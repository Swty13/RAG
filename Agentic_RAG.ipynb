{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70da8516",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "\n",
    "Agentic RAG describes an AI agent-based implementation of RAG.\n",
    "\n",
    "\n",
    "## Why Agentic RAG ?\n",
    "Typical RAG applications have two considerable limitations:\n",
    "\n",
    "1. The naive RAG pipeline only considers one external knowledge source. However, some solutions might require two external knowledge sources, and some solutions might require external tools and APIs, such as web searches.\n",
    "2. They are a one-shot solution, which means that context is retrieved once. There is no reasoning or validation over the quality of the retrieved context.\n",
    "\n",
    "The RAG agent can then reason and act over the following example retrieval scenarios:\n",
    "\n",
    "1. Decide whether to retrieve information or not\n",
    "2. Decide which tool to use to retrieve relevant information\n",
    "3. Formulate the query itself\n",
    "4. Evaluate the retrieved context and decide whether it needs to re-retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529d2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama_index  llama-index-embeddings-langchain  llama-index-llms-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8275b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"2005.14165v4.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "### Define LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_call import *\n",
    "chat_llm=get_llm()\n",
    "embedding_model=get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.llm = chat_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "### Define Summary Index and Vector Index over the Same Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "### Define Query Engines and Set Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 285,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Language Models are Few-Shot Learners\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the Language Models are Few-Shot Learners paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "### Define Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the summary of the Language Models are Few-Shot Learners this paper in less than 100 words?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ba5bb",
   "metadata": {},
   "source": [
    "`Response `\n",
    "\n",
    "Selecting query engine 0:\n",
    "\n",
    "Choice 1 is directly related to summarization questions about the 'Language Models are Few-Shot Learners' paper..\n",
    "The paper \"Language Models are Few-Shot Learners\" demonstrates that large-scale language models, such as GPT-3 with 175 billion parameters, can perform a wide range of tasks with minimal task-specific training. By leveraging few-shot, one-shot, and zero-shot learning, these models achieve competitive performance across various benchmarks, including translation, question answering, and commonsense reasoning. The study highlights the potential of these models to generalize from limited data, reducing the need for extensive fine-tuning, while also addressing issues like bias, fairness, and energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"How GPT2 is diiferent from GPT3?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7552c",
   "metadata": {},
   "source": [
    "`Response `\n",
    "\n",
    "Selecting query engine 1: \n",
    "\n",
    "The question 'How GPT2 is different from GPT3?' requires retrieving specific context from the Language Models are Few-Shot Learners paper, which is best addressed by choice 2..\n",
    "GPT-3 differs from GPT-2 in several key ways. Firstly, GPT-3 has a significantly larger dataset and model size, about two orders of magnitude larger than those used for GPT-2. This includes a large amount of Common Crawl data, which increases the potential for contamination and memorization. Despite this, GPT-3 does not overfit its training set by a significant amount. Additionally, GPT-3's training data includes 7% of text in languages other than English, whereas GPT-2 primarily used an English-only dataset due to capacity concerns. GPT-3 also shows improved performance in various tasks, including translation and question answering, and its performance scales smoothly with model size, indicating that it continues to absorb more knowledge as its capacity increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
