{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid RAG Implementation\n",
    "\n",
    "## Overview\n",
    "Hybrid RAG combines vector-based similarity search with graph-based relationship retrieval. This approach leverages the strengths of both methods: vector search for semantic similarity and graph traversal for contextual relationships.\n",
    "\n",
    "### Key Components:\n",
    "- **Vector Retrieval**: Traditional embedding-based similarity search\n",
    "- **Graph Retrieval**: Entity and relationship-based context\n",
    "- **Result Fusion**: Intelligent combination of both retrieval methods\n",
    "- **Unified Ranking**: Score and rank results from both sources\n",
    "\n",
    "### Use Cases:\n",
    "- Enterprise knowledge management systems\n",
    "- Scientific literature databases\n",
    "- Legal document analysis\n",
    "- Multi-domain information retrieval\n",
    "\n",
    "### Analogy:\n",
    "Like using both Google search (semantic similarity) and Wikipedia links (relationship connections) together to find comprehensive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community\n",
    "!pip install -q networkx spacy transformers\n",
    "!pip install -q sentence-transformers chromadb\n",
    "!pip install -q pandas numpy matplotlib scikit-learn\n",
    "\n",
    "# Download spaCy model for NLP\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorRetriever:\n",
    "    def __init__(self, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        self.vectorstore = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_documents(self, documents: List[str]):\n",
    "        \"\"\"Add documents to vector store\"\"\"\n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        docs = []\n",
    "        for i, doc_text in enumerate(documents):\n",
    "            chunks = text_splitter.split_text(doc_text)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                docs.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"doc_id\": i, \"chunk_id\": j, \"source\": f\"doc_{i}_chunk_{j}\"}\n",
    "                ))\n",
    "        \n",
    "        self.documents = docs\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=\"./vector_db\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Added {len(docs)} document chunks to vector store\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Retrieve similar documents using vector search\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        results = self.vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "        \n",
    "        retrieved_docs = []\n",
    "        for doc, score in results:\n",
    "            retrieved_docs.append({\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"score\": float(score),\n",
    "                \"source\": \"vector\"\n",
    "            })\n",
    "        \n",
    "        return retrieved_docs\n",
    "\n",
    "# Initialize vector retriever\n",
    "vector_retriever = VectorRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Knowledge Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRetriever:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.entity_docs = {}  # Map entities to document content\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        self.nlp = nlp\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'EVENT', 'WORK_OF_ART']:\n",
    "                entities.append({\n",
    "                    'text': ent.text.strip(),\n",
    "                    'label': ent.label_,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char\n",
    "                })\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def build_graph(self, documents: List[str]):\n",
    "        \"\"\"Build knowledge graph from documents\"\"\"\n",
    "        for doc_id, document in enumerate(documents):\n",
    "            entities = self.extract_entities(document)\n",
    "            \n",
    "            # Add entities to graph\n",
    "            for entity in entities:\n",
    "                entity_text = entity['text']\n",
    "                if not self.graph.has_node(entity_text):\n",
    "                    # Create embedding for entity\n",
    "                    entity_embedding = self.embeddings.embed_query(entity_text)\n",
    "                    \n",
    "                    self.graph.add_node(\n",
    "                        entity_text,\n",
    "                        entity_type=entity['label'],\n",
    "                        embedding=entity_embedding,\n",
    "                        documents=set([doc_id])\n",
    "                    )\n",
    "                    self.entity_docs[entity_text] = [document]\n",
    "                else:\n",
    "                    # Add document to existing entity\n",
    "                    self.graph.nodes[entity_text]['documents'].add(doc_id)\n",
    "                    if entity_text not in self.entity_docs:\n",
    "                        self.entity_docs[entity_text] = []\n",
    "                    self.entity_docs[entity_text].append(document)\n",
    "            \n",
    "            # Create relationships between entities in the same document\n",
    "            for i, ent1 in enumerate(entities):\n",
    "                for ent2 in entities[i+1:]:\n",
    "                    if ent1['text'] != ent2['text']:\n",
    "                        # Add bidirectional edges\n",
    "                        self.graph.add_edge(\n",
    "                            ent1['text'], ent2['text'],\n",
    "                            relation='co-occurs',\n",
    "                            document_id=doc_id,\n",
    "                            weight=1.0\n",
    "                        )\n",
    "                        self.graph.add_edge(\n",
    "                            ent2['text'], ent1['text'],\n",
    "                            relation='co-occurs',\n",
    "                            document_id=doc_id,\n",
    "                            weight=1.0\n",
    "                        )\n",
    "        \n",
    "        print(f\"Built graph with {self.graph.number_of_nodes()} entities and {self.graph.number_of_edges()} relationships\")\n",
    "    \n",
    "    def find_relevant_entities(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find entities most similar to query\"\"\"\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        entity_scores = []\n",
    "        for entity in self.graph.nodes():\n",
    "            node_data = self.graph.nodes[entity]\n",
    "            if 'embedding' in node_data:\n",
    "                entity_embedding = node_data['embedding']\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = cosine_similarity(\n",
    "                    [query_embedding], [entity_embedding]\n",
    "                )[0][0]\n",
    "                \n",
    "                entity_scores.append((entity, similarity))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        entity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return entity_scores[:top_k]\n",
    "    \n",
    "    def expand_entities(self, entities: List[str], max_hops: int = 1) -> List[str]:\n",
    "        \"\"\"Expand entities by following graph relationships\"\"\"\n",
    "        expanded = set(entities)\n",
    "        \n",
    "        for entity in entities:\n",
    "            if entity in self.graph:\n",
    "                # Get neighbors within max_hops\n",
    "                neighbors = []\n",
    "                current_level = {entity}\n",
    "                \n",
    "                for hop in range(max_hops):\n",
    "                    next_level = set()\n",
    "                    for node in current_level:\n",
    "                        if node in self.graph:\n",
    "                            next_level.update(self.graph.neighbors(node))\n",
    "                    current_level = next_level - expanded\n",
    "                    expanded.update(current_level)\n",
    "        \n",
    "        return list(expanded)\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant information using graph traversal\"\"\"\n",
    "        # Find relevant entities\n",
    "        relevant_entities = self.find_relevant_entities(query, top_k)\n",
    "        \n",
    "        # Expand entities\n",
    "        entity_names = [entity for entity, score in relevant_entities]\n",
    "        expanded_entities = self.expand_entities(entity_names, max_hops=1)\n",
    "        \n",
    "        # Collect documents for entities\n",
    "        retrieved_docs = []\n",
    "        entity_scores = dict(relevant_entities)\n",
    "        \n",
    "        for entity in expanded_entities:\n",
    "            if entity in self.entity_docs:\n",
    "                for doc_content in self.entity_docs[entity]:\n",
    "                    score = entity_scores.get(entity, 0.5)  # Default score for expanded entities\n",
    "                    retrieved_docs.append({\n",
    "                        \"content\": doc_content,\n",
    "                        \"metadata\": {\"entity\": entity, \"source\": \"graph\"},\n",
    "                        \"score\": score,\n",
    "                        \"source\": \"graph\",\n",
    "                        \"entity\": entity\n",
    "                    })\n",
    "        \n",
    "        return retrieved_docs\n",
    "\n",
    "# Initialize graph retriever\n",
    "graph_retriever = GraphRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hybrid Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    def __init__(self, vector_retriever, graph_retriever, \n",
    "                 vector_weight=0.6, graph_weight=0.4):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.graph_retriever = graph_retriever\n",
    "        self.vector_weight = vector_weight\n",
    "        self.graph_weight = graph_weight\n",
    "    \n",
    "    def normalize_scores(self, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Normalize scores to [0, 1] range\"\"\"\n",
    "        if not results:\n",
    "            return results\n",
    "        \n",
    "        scores = [r['score'] for r in results]\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        \n",
    "        if max_score == min_score:\n",
    "            for result in results:\n",
    "                result['normalized_score'] = 1.0\n",
    "        else:\n",
    "            for result in results:\n",
    "                result['normalized_score'] = (\n",
    "                    result['score'] - min_score\n",
    "                ) / (max_score - min_score)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def deduplicate_results(self, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Remove duplicate content while preserving best scores\"\"\"\n",
    "        seen_content = {}\n",
    "        deduplicated = []\n",
    "        \n",
    "        for result in results:\n",
    "            content = result['content'].strip()\n",
    "            content_hash = hash(content)\n",
    "            \n",
    "            if content_hash not in seen_content:\n",
    "                seen_content[content_hash] = result\n",
    "                deduplicated.append(result)\n",
    "            else:\n",
    "                # Keep result with higher score\n",
    "                if result['final_score'] > seen_content[content_hash]['final_score']:\n",
    "                    # Remove old result\n",
    "                    deduplicated.remove(seen_content[content_hash])\n",
    "                    seen_content[content_hash] = result\n",
    "                    deduplicated.append(result)\n",
    "        \n",
    "        return deduplicated\n",
    "    \n",
    "    def fuse_results(self, vector_results: List[Dict], \n",
    "                    graph_results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Fuse and rank results from both retrievers\"\"\"\n",
    "        # Normalize scores within each result set\n",
    "        vector_results = self.normalize_scores(vector_results)\n",
    "        graph_results = self.normalize_scores(graph_results)\n",
    "        \n",
    "        # Combine results with weighted scores\n",
    "        all_results = []\n",
    "        \n",
    "        for result in vector_results:\n",
    "            result['final_score'] = (\n",
    "                result['normalized_score'] * self.vector_weight\n",
    "            )\n",
    "            all_results.append(result)\n",
    "        \n",
    "        for result in graph_results:\n",
    "            result['final_score'] = (\n",
    "                result['normalized_score'] * self.graph_weight\n",
    "            )\n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        deduplicated_results = self.deduplicate_results(all_results)\n",
    "        \n",
    "        # Sort by final score\n",
    "        deduplicated_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        \n",
    "        return deduplicated_results\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, \n",
    "                vector_k: int = 8, graph_k: int = 5) -> Dict:\n",
    "        \"\"\"Main hybrid retrieval method\"\"\"\n",
    "        # Get results from both retrievers\n",
    "        vector_results = self.vector_retriever.retrieve(query, vector_k)\n",
    "        graph_results = self.graph_retriever.retrieve(query, graph_k)\n",
    "        \n",
    "        # Fuse results\n",
    "        fused_results = self.fuse_results(vector_results, graph_results)\n",
    "        \n",
    "        # Return top_k results\n",
    "        final_results = fused_results[:top_k]\n",
    "        \n",
    "        return {\n",
    "            'results': final_results,\n",
    "            'vector_count': len(vector_results),\n",
    "            'graph_count': len(graph_results),\n",
    "            'fused_count': len(fused_results),\n",
    "            'final_count': len(final_results)\n",
    "        }\n",
    "\n",
    "# Initialize hybrid retriever (will be set up after loading data)\n",
    "hybrid_retriever = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Data and System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector store...\n",
      "Added 4 document chunks to vector store\n",
      "Building knowledge graph...\n",
      "Built graph with 11 entities and 30 relationships\n",
      "Initializing hybrid retriever...\n",
      "\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Sample documents covering AI/ML topics\n",
    "sample_documents = [\n",
    "    \"\"\"OpenAI developed GPT-3, a large language model with 175 billion parameters. \n",
    "    The model was trained by a team including Alec Radford, Jeffrey Wu, and Ilya Sutskever. \n",
    "    GPT-3 uses the transformer architecture and was released in June 2020. It demonstrated \n",
    "    remarkable capabilities in text generation, translation, and reasoning tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"Google's BERT (Bidirectional Encoder Representations from Transformers) was introduced \n",
    "    by Jacob Devlin and his team in 2018. BERT revolutionized natural language processing \n",
    "    by using bidirectional training of transformers. The model achieved state-of-the-art \n",
    "    results on eleven natural language processing tasks, including question answering.\"\"\",\n",
    "    \n",
    "    \"\"\"The transformer architecture was introduced in the paper 'Attention is All You Need' \n",
    "    by Ashish Vaswani and colleagues at Google Brain. Published in 2017, this architecture \n",
    "    eliminated the need for recurrent neural networks and became the foundation for \n",
    "    models like BERT, GPT, and T5. The key innovation was the self-attention mechanism.\"\"\",\n",
    "    \n",
    "    \"\"\"Meta AI released LLaMA (Large Language Model Meta AI) in 2023. The LLaMA models \n",
    "    range from 7B to 65B parameters and were designed to be more efficient than GPT-3. \n",
    "    Hugo Touvron led the development team. LLaMA demonstrated that smaller models \n",
    "    could achieve competitive performance when trained on high-quality data.\"\"\"\n",
    "]\n",
    "\n",
    "print(\"Setting up vector store...\")\n",
    "vector_retriever.add_documents(sample_documents)\n",
    "\n",
    "print(\"Building knowledge graph...\")\n",
    "graph_retriever.build_graph(sample_documents)\n",
    "\n",
    "print(\"Initializing hybrid retriever...\")\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    vector_retriever, \n",
    "    graph_retriever,\n",
    "    vector_weight=0.6,\n",
    "    graph_weight=0.4\n",
    ")\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Fixed System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query: What is GPT-3 and who developed it?\n",
      "==================================================\n",
      "Retrieval Statistics:\n",
      "- Vector results: 4\n",
      "- Graph results: 14\n",
      "- Final results: 3\n",
      "\n",
      "Top Results:\n",
      "1. [VECTOR] Score: 0.600\n",
      "   Content: Google's BERT (Bidirectional Encoder Representations from Transformers) was introduced \n",
      "    by Jacob...\n",
      "\n",
      "2. [VECTOR] Score: 0.478\n",
      "   Content: The transformer architecture was introduced in the paper 'Attention is All You Need' \n",
      "    by Ashish ...\n",
      "\n",
      "3. [GRAPH] Score: 0.400\n",
      "   Entity: GPT-3\n",
      "   Content: OpenAI developed GPT-3, a large language model with 175 billion parameters. \n",
      "    The model was train...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the hybrid retrieval system\n",
    "test_query = \"What is GPT-3 and who developed it?\"\n",
    "\n",
    "print(f\"Testing query: {test_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get hybrid results\n",
    "result = hybrid_retriever.retrieve(test_query, top_k=3)\n",
    "\n",
    "print(f\"Retrieval Statistics:\")\n",
    "print(f\"- Vector results: {result['vector_count']}\")\n",
    "print(f\"- Graph results: {result['graph_count']}\")\n",
    "print(f\"- Final results: {result['final_count']}\")\n",
    "\n",
    "print(\"\\nTop Results:\")\n",
    "for i, res in enumerate(result['results'], 1):\n",
    "    print(f\"{i}. [{res['source'].upper()}] Score: {res['final_score']:.3f}\")\n",
    "    if 'entity' in res:\n",
    "        print(f\"   Entity: {res['entity']}\")\n",
    "    print(f\"   Content: {res['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Fixed Issues:**\n",
    "1. Corrected JSON syntax errors in notebook structure\n",
    "2. Proper metadata formatting\n",
    "3. Valid Jupyter notebook format\n",
    "\n",
    "✅ **Hybrid RAG Features:**\n",
    "- Vector-based semantic search\n",
    "- Graph-based entity relationships\n",
    "- Intelligent result fusion\n",
    "- Configurable weight system\n",
    "- Performance analysis tools\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
