{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {},
   "source": [
    "# Re-ranking Retrieval RAG\n",
    "\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to implement a Retrieval-Augmented Generation (RAG) system with reranking capabilities using LangChain. The system enhances the quality of retrieved documents through a two-stage process: initial retrieval followed by reranking.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "##### 1.Document Processing\n",
    "The system processes documents through multiple stages:\n",
    "- Document loading and text splitting\n",
    "- Embedding generation using HuggingFace sentence transformers\n",
    "- Vector store creation with FAISS for efficient similarity search\n",
    "\n",
    "##### 2. Reranking System\n",
    "Implements a sophisticated retrieval process:\n",
    "- Initial retrieval using FAISS vector similarity\n",
    "- Document reranking using Cohere's rerank model\n",
    "- Top-K filtering for most relevant context selection\n",
    "\n",
    "##### 3. Response Generation\n",
    "Leverages modern language models for response generation:\n",
    "- Uses GPT-4 for text generation\n",
    "- Implements contextual prompt templates\n",
    "- Processes retrieved context for coherent responses\n",
    "\n",
    "\n",
    "### Enviornment\n",
    "\n",
    "`(1) Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-community faiss-cpu sentence-transformers cohere huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
   "metadata": {},
   "source": [
    "### Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c282c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "441fd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "    \n",
    "    def load_and_split(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Load and split documents\"\"\"\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        split_docs = self.text_splitter.split_documents(documents)\n",
    "        return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5c5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReranker:\n",
    "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "    def compute_relevance_scores(self, query: str, passages: List[str]) -> List[float]:\n",
    "        \"\"\"Compute relevance scores between query and passages\"\"\"\n",
    "        pairs = [[query, passage] for passage in passages]\n",
    "        features = self.tokenizer(\n",
    "            pairs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = self.model(**features).logits.squeeze(-1)\n",
    "        \n",
    "        return scores.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd348d23",
   "metadata": {},
   "source": [
    "### Load Embedding and Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_call import *\n",
    "chat_llm=get_llm()\n",
    "embedding_model=get_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d",
   "metadata": {},
   "source": [
    "### Enhanced Retriever with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRetriever:\n",
    "    def __init__(self):\n",
    "        self.embeddings = embedding_model\n",
    "        self.vectorstore = None\n",
    "        self.reranker = CustomReranker()\n",
    "        \n",
    "    def index_documents(self, documents: List[Document]):\n",
    "        \"\"\"Create FAISS index from documents\"\"\"\n",
    "        self.vectorstore = FAISS.from_documents(documents, self.embeddings)\n",
    "    \n",
    "    def retrieve_and_rerank(self, query: str, k: int = 5, rerank_k: int = 3) :\n",
    "        \"\"\"Retrieve documents and rerank them\"\"\"\n",
    "        # Initial retrieval\n",
    "        docs = self.vectorstore.similarity_search(query, k=k)\n",
    "        \n",
    "        # Extract passages for reranking\n",
    "        passages = [doc.page_content for doc in docs]\n",
    "        \n",
    "        # Compute relevance scores\n",
    "        relevance_scores = self.reranker.compute_relevance_scores(query, passages)\n",
    "        \n",
    "        # Sort documents by relevance score\n",
    "        scored_docs = list(zip(docs, relevance_scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top rerank_k documents\n",
    "        return [doc for doc, _ in scored_docs[:rerank_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba890329-1411-4922-bd27-fe0490dd1208",
   "metadata": {},
   "source": [
    "### Query Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_query_generator():\n",
    "    llm = chat_llm\n",
    "    \n",
    "    prompt_template = \"\"\"Generate three different versions of the given question to retrieve relevant documents. \n",
    "    Make the questions diverse while preserving the original meaning.\n",
    "    \n",
    "    Original Question: {query}\n",
    "    \n",
    "    Generated Questions:\n",
    "    1.\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e8f13",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7096ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipelineWithReranking:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.retriever = EnhancedRetriever()\n",
    "        self.query_generator = setup_query_generator()\n",
    "        \n",
    "    def index_documents(self, file_path: str):\n",
    "        \"\"\"Process and index documents\"\"\"\n",
    "        documents = self.document_processor.load_and_split(file_path)\n",
    "        self.retriever.index_documents(documents)\n",
    "        \n",
    "    def process_query(self, query: str, k: int = 5, rerank_k: int = 3):\n",
    "        \"\"\"Process query with re-ranking\"\"\"\n",
    "        # Generate multiple queries\n",
    "        query_variations = self.query_generator.run(query).split(\"\\n\")\n",
    "        \n",
    "        # Get results for each query variation\n",
    "        all_docs = []\n",
    "        for q in query_variations:\n",
    "            if q.strip():\n",
    "                docs = self.retriever.retrieve_and_rerank(q, k=k, rerank_k=rerank_k)\n",
    "                all_docs.extend(docs)\n",
    "        \n",
    "        # Remove duplicates and get final top documents\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in all_docs:\n",
    "            if doc.page_content not in seen:\n",
    "                seen.add(doc.page_content)\n",
    "                unique_docs.append(doc)\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"query_variations\": query_variations,\n",
    "            \"retrieved_documents\": unique_docs[:rerank_k]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27aa80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = RAGPipelineWithReranking()\n",
    "\n",
    "file_path=\"./papers/2005.14165v4.pdf\"\n",
    "# Index documents\n",
    "pipeline.index_documents(file_path)\n",
    "\n",
    "# Example query\n",
    "query = \"What is the difference between GPT2 and GPT3?\"\n",
    "\n",
    "# Process query\n",
    "result = pipeline.process_query(query)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Query: {result['original_query']}\\n\")\n",
    "print(\"Query Variations:\")\n",
    "for q in result['query_variations']:\n",
    "        if q.strip():\n",
    "                print(f\"- {q.strip()}\")\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for i, doc in enumerate(result['retrieved_documents'], 1):\n",
    "        print(f\"\\n{i}. {doc.page_content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
